{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.contrib.layers.python.layers import regularizers\n",
    "import numpy as np\n",
    "slim = tf.contrib.slim\n",
    "tf.reset_default_graph()\n",
    "trunc_normal = lambda stddev: init_ops.truncated_normal_initializer(0.0, stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Contants\n",
    "image_channels = 3\n",
    "time_frames_to_consider = 4\n",
    "heigth_train= 32\n",
    "width_train= 32\n",
    "heigth_test= 210\n",
    "width_test= 160\n",
    "# +1 for input from previous layer !\n",
    "scale_level_feature_maps= [[128, 256, 128, 3],\n",
    "                           [128, 256, 128, 3],\n",
    "                           [128, 256, 512, 256, 128, 3],\n",
    "                           [128, 256, 512, 256, 128, 3]]\n",
    "# as size of image increase in scaling ... conv layer increases !\n",
    "scale_level_kernel_size = [ \n",
    "                            [3, 3, 3, 3],\n",
    "                            [5, 3, 3, 5],\n",
    "                            [5, 3, 3, 3, 3, 5],\n",
    "                            [7, 5, 5, 5, 5, 7]\n",
    "                          ]\n",
    "# regularizer !\n",
    "l2_val = 0.00005\n",
    "# Adam optimizer !\n",
    "adam_learning_rate = 0.0004\n",
    "\n",
    "## ===================  COPIED CODE ==========================\n",
    "#\n",
    "#  TENSORBOARD VISUALIZATION FOR SHARPNESS AND (Peak Signal to Noise Ratio){PSNR}\n",
    "#\n",
    "\n",
    "def psnr_error(gen_frames, gt_frames):\n",
    "    \"\"\"\n",
    "    Computes the Peak Signal to Noise Ratio error between the generated images and the ground\n",
    "    truth images.\n",
    "    @param gen_frames: A tensor of shape [batch_size, height, width, 3]. The frames generated by the\n",
    "                       generator model.\n",
    "    @param gt_frames: A tensor of shape [batch_size, height, width, 3]. The ground-truth frames for\n",
    "                      each frame in gen_frames.\n",
    "    @return: A scalar tensor. The mean Peak Signal to Noise Ratio error over each frame in the\n",
    "             batch.\n",
    "    \"\"\"\n",
    "    shape = tf.shape(gen_frames)\n",
    "    num_pixels = tf.to_float(shape[1] * shape[2] * shape[3])\n",
    "    square_diff = tf.square(gt_frames - gen_frames)\n",
    "\n",
    "    batch_errors = 10 * log10(1 / ((1 / num_pixels) * tf.reduce_sum(square_diff, [1, 2, 3])))\n",
    "    return tf.reduce_mean(batch_errors)\n",
    "\n",
    "def sharp_diff_error(gen_frames, gt_frames):\n",
    "    \"\"\"\n",
    "    Computes the Sharpness Difference error between the generated images and the ground truth\n",
    "    images.\n",
    "    @param gen_frames: A tensor of shape [batch_size, height, width, 3]. The frames generated by the\n",
    "                       generator model.\n",
    "    @param gt_frames: A tensor of shape [batch_size, height, width, 3]. The ground-truth frames for\n",
    "                      each frame in gen_frames.\n",
    "    @return: A scalar tensor. The Sharpness Difference error over each frame in the batch.\n",
    "    \"\"\"\n",
    "    shape = tf.shape(gen_frames)\n",
    "    num_pixels = tf.to_float(shape[1] * shape[2] * shape[3])\n",
    "\n",
    "    # gradient difference\n",
    "    # create filters [-1, 1] and [[1],[-1]] for diffing to the left and down respectively.\n",
    "    # TODO: Could this be simplified with one filter [[-1, 2], [0, -1]]?\n",
    "    pos = tf.constant(np.identity(3), dtype=tf.float32)\n",
    "    neg = -1 * pos\n",
    "    filter_x = tf.expand_dims(tf.pack([neg, pos]), 0)  # [-1, 1]\n",
    "    filter_y = tf.pack([tf.expand_dims(pos, 0), tf.expand_dims(neg, 0)])  # [[1],[-1]]\n",
    "    strides = [1, 1, 1, 1]  # stride of (1, 1)\n",
    "    padding = 'SAME'\n",
    "\n",
    "    gen_dx = tf.abs(tf.nn.conv2d(gen_frames, filter_x, strides, padding=padding))\n",
    "    gen_dy = tf.abs(tf.nn.conv2d(gen_frames, filter_y, strides, padding=padding))\n",
    "    gt_dx = tf.abs(tf.nn.conv2d(gt_frames, filter_x, strides, padding=padding))\n",
    "    gt_dy = tf.abs(tf.nn.conv2d(gt_frames, filter_y, strides, padding=padding))\n",
    "\n",
    "    gen_grad_sum = gen_dx + gen_dy\n",
    "    gt_grad_sum = gt_dx + gt_dy\n",
    "\n",
    "    grad_diff = tf.abs(gt_grad_sum - gen_grad_sum)\n",
    "\n",
    "    batch_errors = 10 * log10(1 / ((1 / num_pixels) * tf.reduce_sum(grad_diff, [1, 2, 3])))\n",
    "    return tf.reduce_mean(batch_errors)\n",
    "\n",
    "## =================== COPIED CODE ENDS ======================\n",
    "\n",
    "\n",
    "def l2_loss(generated_frames, expected_frames):\n",
    "    losses = []\n",
    "    for each_scale_gen_frames, each_scale_exp_frames in zip(generated_frames, expected_frames):\n",
    "        losses.append(tf.nn.l2_loss(tf.subtract(each_scale_gen_frames, each_scale_exp_frames)))\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.stack(losses))\n",
    "    return loss\n",
    "\n",
    "def gdl_loss(generated_frames, expected_frames, alpha=2):\n",
    "    \"\"\"\n",
    "    difference with side pixel and below pixel\n",
    "    \"\"\"\n",
    "    scale_losses = []\n",
    "    for i in xrange(len(generated_frames)):\n",
    "        # create filters [-1, 1] and [[1],[-1]] for diffing to the left and down respectively.\n",
    "        pos = tf.constant(np.identity(3), dtype=tf.float32)\n",
    "        neg = -1 * pos\n",
    "        filter_x = tf.expand_dims(tf.stack([neg, pos]), 0)  # [-1, 1]\n",
    "        filter_y = tf.stack([tf.expand_dims(pos, 0), tf.expand_dims(neg, 0)])  # [[1],[-1]]\n",
    "        strides = [1, 1, 1, 1]  # stride of (1, 1)\n",
    "        padding = 'SAME'\n",
    "\n",
    "        gen_dx = tf.abs(tf.nn.conv2d(generated_frames[i], filter_x, strides, padding=padding))\n",
    "        gen_dy = tf.abs(tf.nn.conv2d(generated_frames[i], filter_y, strides, padding=padding))\n",
    "        gt_dx = tf.abs(tf.nn.conv2d(expected_frames[i], filter_x, strides, padding=padding))\n",
    "        gt_dy = tf.abs(tf.nn.conv2d(expected_frames[i], filter_y, strides, padding=padding))\n",
    "\n",
    "        grad_diff_x = tf.abs(gt_dx - gen_dx)\n",
    "        grad_diff_y = tf.abs(gt_dy - gen_dy)\n",
    "\n",
    "        scale_losses.append(tf.reduce_sum((grad_diff_x ** alpha + grad_diff_y ** alpha)))\n",
    "\n",
    "    # condense into one tensor and avg\n",
    "    return tf.reduce_mean(tf.stack(scale_losses))\n",
    "\n",
    "def total_loss(generated_frames, expected_frames, loss_from_disc, lambda_gdl=1.0, lambda_l2=1.0, lambda_disc=1.0):\n",
    "    total_loss_cal = (lambda_gdl * gdl_loss(generated_frames, expected_frames) + \n",
    "                     lambda_l2 * l2_loss(generated_frames, expected_frames)+\n",
    "                     lambda_disc * loss_from_disc)\n",
    "    return total_loss_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GenerativeNetwork:\n",
    "    def __init__(self, heigth_train, width_train, heigth_test, width_test, scale_level_feature_maps, scale_level_kernel_size):\n",
    "        \n",
    "        self.heigth_train = heigth_train\n",
    "        self.width_train = width_train\n",
    "        self.heigth_test = heigth_test\n",
    "        self.width_test = width_test\n",
    "\n",
    "        self.scale_level_feature_maps = scale_level_feature_maps\n",
    "        self.scale_level_kernel_size = scale_level_kernel_size\n",
    "        self.len_scale = len(self.scale_level_kernel_size)\n",
    "        assert len(self.scale_level_feature_maps) == len(self.scale_level_kernel_size), \"Length should be equal !\"\n",
    "        \n",
    "        # Placeholders for inputs and outputs ... !\n",
    "        self.input_train = tf.placeholder(dtype=tf.float32, shape=[None, self.heigth_train, self.width_train, time_frames_to_consider * image_channels])\n",
    "        self.output_train = tf.placeholder(dtype=tf.float32, shape=[None, self.heigth_train, self.width_train, image_channels])\n",
    "        self.input_test = tf.placeholder(dtype=tf.float32, shape=[None, self.heigth_test, self.width_test, time_frames_to_consider * image_channels])\n",
    "        self.output_test = tf.placeholder(dtype=tf.float32, shape=[None, self.heigth_test, self.width_test, image_channels])\n",
    "        self.loss_from_disc = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "        \n",
    "        self.each_scale_predication_train = []\n",
    "        self.each_scale_ground_truth_train = []\n",
    "        self.each_scale_predication_test = []\n",
    "        self.each_scale_ground_truth_test = []\n",
    "        \n",
    "        self.create_graph(self.input_train, self.output_train, heigth_train, width_train, \n",
    "                          self.each_scale_predication_train, \n",
    "                          self.each_scale_ground_truth_train,\n",
    "                          reuse=None)\n",
    "        \n",
    "        # reuse graph at time of test !\n",
    "        self.create_graph(self.input_train, self.output_train, heigth_test, width_test, \n",
    "                          self.each_scale_predication_test,\n",
    "                          self.each_scale_ground_truth_test,\n",
    "                          reuse=True)\n",
    "        \n",
    "        self.loss()\n",
    "        \n",
    "        # print self.each_scale_predication_train\n",
    "        # print self.each_scale_ground_truth_train\n",
    "        # print self.each_scale_predication_test\n",
    "        # print self.each_scale_ground_truth_test\n",
    "        \n",
    "    def rescale_image(self, scaling_factor, heigth, width, input_data, ground_truths, last_generated_frame):\n",
    "        \"\"\"\n",
    "        scaling_factor, heigth, width = values\n",
    "        input_data, ground_truths = Tensors\n",
    "        \"\"\"\n",
    "        rescaled_heigth = int(scaling_factor * heigth)\n",
    "        rescaled_width = int(scaling_factor * width)\n",
    "        assert rescaled_heigth != 0 and rescaled_width != 0, \"scaling factor should not be zero !\"\n",
    "        input_reshaped = tf.image.resize_images(input_data, [rescaled_heigth, rescaled_width])\n",
    "        ground_truths_reshaped = tf.image.resize_images(ground_truths, [rescaled_heigth, rescaled_width])\n",
    "        last_generated_frame_reshaped = None\n",
    "        if last_generated_frame!=None:\n",
    "            last_generated_frame_reshaped = tf.image.resize_images(last_generated_frame, [rescaled_heigth, rescaled_width])\n",
    "        return (input_reshaped, ground_truths_reshaped, last_generated_frame_reshaped)\n",
    "    \n",
    "    def create_graph(self, input_data, ground_truths, heigth, width, \n",
    "                     predicated_at_each_scale_tensor, ground_truth_at_each_scale_tensor, reuse):\n",
    "                \n",
    "        # for each scale ... \n",
    "        for each_scale in range(self.len_scale):\n",
    "            conv_counter = 0 \n",
    "            with tf.variable_scope('scale_'+str(each_scale),reuse=reuse):\n",
    "                # scaling create [1/64, 1/32, 1/16, 1/4]\n",
    "                scaling_factor = 1.0 / (2**(self.len_scale - 1 - each_scale))\n",
    "                last_generated_frame = None\n",
    "                if each_scale > 0:\n",
    "                    last_generated_frame = predicated_at_each_scale_tensor[each_scale-1]\n",
    "                \n",
    "                input_reshaped, ground_truths_reshaped, last_generated_frame_reshaped = self.rescale_image(scaling_factor, heigth, width, input_data, ground_truths, last_generated_frame)\n",
    "                \n",
    "                # append last scale output \n",
    "                if each_scale > 0:\n",
    "                    input_reshaped = tf.concat([input_reshaped, last_generated_frame_reshaped],axis=3)\n",
    "                \n",
    "                # print (input_reshaped, ground_truths_reshaped)\n",
    "                predication = input_reshaped\n",
    "                \n",
    "                # for each conv layers in that scale ... \n",
    "                feature_maps = scale_level_feature_maps[each_scale]\n",
    "                kernel_size = scale_level_kernel_size[each_scale]\n",
    "                \n",
    "                assert len(feature_maps)==len(kernel_size), \"Length should be equal !\"\n",
    "                for index, (each_filter, each_kernel) in enumerate(zip(feature_maps, kernel_size)): \n",
    "                    with tf.variable_scope('conv_'+str(conv_counter),reuse=reuse):\n",
    "                        conv_counter += 1\n",
    "                        activiation = tf.nn.relu\n",
    "                        # last layer tanh !\n",
    "                        if index==(len(kernel_size)-1):\n",
    "                            activiation = tf.nn.tanh\n",
    "                        predication = slim.conv2d(predication, each_filter, [each_kernel, each_kernel], \n",
    "                                              weights_initializer=trunc_normal(0.01),\n",
    "                                              weights_regularizer=regularizers.l2_regularizer(l2_val),\n",
    "                                              activation_fn=activiation)\n",
    "                \n",
    "                        \n",
    "                # APPEND LAST GENERATED FRAME\n",
    "                predicated_at_each_scale_tensor.append(predication)\n",
    "                ground_truth_at_each_scale_tensor.append(ground_truths_reshaped)\n",
    "    \n",
    "    def loss(self):        \n",
    "        # discriminator, gdl and l2 loss !\n",
    "        self.combined_loss = total_loss(self.each_scale_predication_train, self.each_scale_ground_truth_train, self.loss_from_disc)\n",
    "        self.optimizer = tf.train.AdamOptimizer(adam_learning_rate)\n",
    "        global_step = tf.Variable(0,name=\"global_step_var\",trainable=False)\n",
    "        self.step = self.optimizer.minimize(self.combined_loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = GenerativeNetwork(heigth_train, width_train, heigth_test, width_test, scale_level_feature_maps, scale_level_kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "disc_scale_level_feature_maps =   [[64],\n",
    "                                   [64, 128, 128],\n",
    "                                   [128, 256, 256],\n",
    "                                   [128, 256, 512, 128]]\n",
    "# kernel sizes for each convolution of each scale network in the discriminator model\n",
    "disc_scale_level_kernel_size =  [[3],\n",
    "                                [3, 3, 3],\n",
    "                                [5, 5, 5],\n",
    "                                [7, 7, 5, 5]]\n",
    "# layer sizes for each fully-connected layer of each scale network in the discriminator model\n",
    "# layer connecting conv to fully-connected is dynamically generated when creating the model\n",
    "disc_fc_layer_units     = [[512, 256, 1],\n",
    "                          [1024, 512, 1],\n",
    "                          [1024, 512, 1],\n",
    "                          [1024, 512, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScaleBasedDiscriminator:\n",
    "    def __init__(self, heigth, width, kernel_size, feature_maps, fc_layer_units, scale_number):\n",
    "        assert len(feature_maps)==len(kernel_size), \"Length should be equal !\"\n",
    "        self.heigth = heigth\n",
    "        self.width = width\n",
    "        self.kernel_size = kernel_size\n",
    "        self.feature_maps = feature_maps\n",
    "        self.fc_layer_units = fc_layer_units\n",
    "        self.scale_number = scale_number\n",
    "        self.input = tf.placeholder(dtype=tf.float32, shape=[None, self.heigth, self.width, image_channels])\n",
    "        self.create_graph()\n",
    "        \n",
    "    def create_graph(self):\n",
    "        predication = self.input\n",
    "        with tf.variable_scope('discriminator_scale_'+str(self.scale_number)):\n",
    "            conv_counter = 0\n",
    "            for index, (each_filter, each_kernel) in enumerate(zip(self.feature_maps, self.kernel_size)):\n",
    "                with tf.variable_scope('conv_'+str(conv_counter)):\n",
    "                    conv_counter += 1\n",
    "                    stride = 1\n",
    "                    # last layer stride 2 ... fc layer weights reduce ...\n",
    "                    if index == (len(self.feature_maps)-1):\n",
    "                        stride = 2\n",
    "                    predication = slim.conv2d(predication, each_filter, [each_kernel, each_kernel],\n",
    "                                              padding = 'VALID',\n",
    "                                              stride = stride,\n",
    "                                              weights_initializer=trunc_normal(0.01),\n",
    "                                              weights_regularizer=regularizers.l2_regularizer(l2_val))\n",
    "                    # print predication\n",
    "            \n",
    "            predication = slim.flatten(predication)\n",
    "            # print predication\n",
    "            \n",
    "            fully_connected_counter = 0\n",
    "            for index, each_layer_units in enumerate(self.fc_layer_units):\n",
    "                with tf.variable_scope('fully_connected'+str(fully_connected_counter)):\n",
    "                    fully_connected_counter += 1\n",
    "                    activation = tf.nn.relu\n",
    "                    # last layer sigmoid !\n",
    "                    if index == (len(self.fc_layer_units)-1):\n",
    "                        activation = tf.nn.sigmoid\n",
    "                    predication = slim.fully_connected(predication, each_layer_units, activation_fn=activation)\n",
    "                    # print predication\n",
    "            # clip value between 0.1 and 0.9\n",
    "            self.predication = tf.clip_by_value(predication, 0.1, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sc = ScaleBasedDiscriminator(32,32,disc_scale_level_kernel_size[0],disc_scale_level_feature_maps[0],disc_fc_layer_units[0],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self, heigth, width, disc_scale_level_feature_maps, disc_scale_level_kernel_size, disc_fc_layer_units):\n",
    "        assert len(disc_scale_level_feature_maps)==len(disc_scale_level_kernel_size), \"Length should be equal !\"\n",
    "        assert len(disc_scale_level_feature_maps)==len(disc_fc_layer_units), \"Length should be equal !\"\n",
    "        \n",
    "        self.heigth = heigth\n",
    "        self.width = width\n",
    "        self.disc_scale_level_feature_maps = disc_scale_level_feature_maps\n",
    "        self.disc_scale_level_kernel_size = disc_scale_level_kernel_size\n",
    "        self.disc_fc_layer_units = disc_fc_layer_units\n",
    "        \n",
    "        # ground truth image \n",
    "        self.ground_truth_images = tf.placeholder(dtype=tf.float32, shape=[None, self.heigth, self.width, image_channels])\n",
    "        # real or fake\n",
    "        self.ground_truth_labels = tf.placeholder(dtype=tf.float32, shape=[None,1])\n",
    "        \n",
    "        self.len_scale = len(self.disc_scale_level_kernel_size)\n",
    "        self.create_graph()\n",
    "        self.loss()\n",
    "        self.scale_images_ground_truth_for_inputs()\n",
    "        \n",
    "    def create_graph(self,):\n",
    "        self.scale_based_discriminators = []\n",
    "        for each_scale, (each_feature_map, each_kernel_size, each_fc_layer) in enumerate(zip(self.disc_scale_level_feature_maps, self.disc_scale_level_kernel_size, self.disc_fc_layer_units)):\n",
    "            # scaling create [1/64, 1/32, 1/16, 1/4]\n",
    "            scaling_factor = 1.0 / (2**(self.len_scale - 1 - each_scale))\n",
    "            rescaled_heigth = int(scaling_factor * self.heigth)\n",
    "            rescaled_width = int(scaling_factor * self.width)\n",
    "            \n",
    "            disc_at_scale = ScaleBasedDiscriminator(heigth=rescaled_heigth,\n",
    "                                                    width=rescaled_width, kernel_size=each_kernel_size, \n",
    "                                                    feature_maps=each_feature_map, \n",
    "                                                    fc_layer_units=each_fc_layer, scale_number=each_scale)\n",
    "            self.scale_based_discriminators.append(disc_at_scale)\n",
    "            \n",
    "        \n",
    "        self.scaled_disc_predication = []\n",
    "        for each_scaled_pred in self.scale_based_discriminators:\n",
    "            self.scaled_disc_predication.append(each_scaled_pred.predication)\n",
    "        # print self.scaled_disc_predication\n",
    "        \n",
    "    def loss(self):\n",
    "        total_loss = []\n",
    "        for each_scaled_op in self.scaled_disc_predication:\n",
    "            # print each_scaled_op, self.ground_truth_labels\n",
    "            curr_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=self.ground_truth_labels, logits=each_scaled_op)\n",
    "            total_loss.append(curr_loss)\n",
    "        \n",
    "        self.dis_loss = tf.reduce_mean(tf.stack(total_loss))\n",
    "        self.optimizer = tf.train.AdamOptimizer(adam_learning_rate)\n",
    "        global_step = tf.Variable(0,name=\"dis_global_step_var\",trainable=False)\n",
    "        self.step = self.optimizer.minimize(self.dis_loss, global_step=global_step)\n",
    "    \n",
    "    def rescale_image(self, scaling_factor, heigth, width, ground_truths):\n",
    "        \"\"\"\n",
    "        scaling_factor, heigth, width = values\n",
    "        input_data, ground_truths = Tensors\n",
    "        \"\"\"\n",
    "        rescaled_heigth = int(scaling_factor * heigth)\n",
    "        rescaled_width = int(scaling_factor * width)\n",
    "        assert rescaled_heigth != 0 and rescaled_width != 0, \"scaling factor should not be zero !\"\n",
    "        ground_truths_reshaped = tf.image.resize_images(ground_truths, [rescaled_heigth, rescaled_width])\n",
    "        return ground_truths_reshaped\n",
    "    \n",
    "    def scale_images_ground_truth_for_inputs(self,):\n",
    "        inputs = []\n",
    "        for each_scale in range(self.len_scale):\n",
    "            scaling_factor = 1.0 / (2**(self.len_scale - 1 - each_scale))\n",
    "            inputs.append(self.rescale_image(scaling_factor, self.heigth, self.width, self.ground_truth_images))\n",
    "        self.rescaled_ground_truth_images = inputs\n",
    "        # print inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:logits.dtype=<dtype: 'float32'>.\n",
      "INFO:tensorflow:multi_class_labels.dtype=<dtype: 'float32'>.\n",
      "INFO:tensorflow:losses.dtype=<dtype: 'float32'>.\n",
      "INFO:tensorflow:logits.dtype=<dtype: 'float32'>.\n",
      "INFO:tensorflow:multi_class_labels.dtype=<dtype: 'float32'>.\n",
      "INFO:tensorflow:losses.dtype=<dtype: 'float32'>.\n",
      "INFO:tensorflow:logits.dtype=<dtype: 'float32'>.\n",
      "INFO:tensorflow:multi_class_labels.dtype=<dtype: 'float32'>.\n",
      "INFO:tensorflow:losses.dtype=<dtype: 'float32'>.\n",
      "INFO:tensorflow:logits.dtype=<dtype: 'float32'>.\n",
      "INFO:tensorflow:multi_class_labels.dtype=<dtype: 'float32'>.\n",
      "INFO:tensorflow:losses.dtype=<dtype: 'float32'>.\n"
     ]
    }
   ],
   "source": [
    "d = Discriminator(64, 64, disc_scale_level_feature_maps, disc_scale_level_kernel_size, disc_fc_layer_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "import scipy\n",
    "x = tf.constant([1.0,0,0,1.,1.,0],dtype=tf.float32)\n",
    "y = tf.stack([tf.constant([0.5]),tf.constant([.9]),tf.constant([.6]),tf.constant([.1]),tf.constant([.8]),tf.constant([.389])])\n",
    "y = tf.reshape(y,[-1,1])\n",
    "sess = tf.Session()\n",
    "print sess.run(tf.losses.sigmoid_cross_entropy(multi_class_labels=x, logits=y))\n",
    "print sess.run(tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=x,logits=y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:logits.dtype=<dtype: 'float32'>.\n",
      "INFO:tensorflow:multi_class_labels.dtype=<dtype: 'float32'>.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes (6, 1) and (6,) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-4d7d083e493c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_class_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pratik/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/losses/losses_impl.pyc\u001b[0m in \u001b[0;36msigmoid_cross_entropy\u001b[0;34m(multi_class_labels, logits, weights, label_smoothing, scope, loss_collection, reduction)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0mmulti_class_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_class_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multi_class_labels.dtype=%s.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m     \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_class_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabel_smoothing\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pratik/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.pyc\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \"\"\"\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (6, 1) and (6,) are incompatible"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
