{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.contrib.layers.python.layers import regularizers\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "tf.reset_default_graph()\n",
    "trunc_normal = lambda stddev: init_ops.truncated_normal_initializer(0.0, stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Contants\n",
    "image_channels = 3\n",
    "time_frames_to_consider = 4\n",
    "heigth_train= 32\n",
    "width_train= 32\n",
    "heigth_test= 210\n",
    "width_test= 160\n",
    "# +1 for input from previous layer !\n",
    "scale_level_feature_maps= [[128, 256, 128, 3],\n",
    "                           [128, 256, 128, 3],\n",
    "                           [128, 256, 512, 256, 128, 3],\n",
    "                           [128, 256, 512, 256, 128, 3]]\n",
    "# as size of image increase in scaling ... conv layer increases !\n",
    "scale_level_kernel_size = [ \n",
    "                            [3, 3, 3, 3],\n",
    "                            [5, 3, 3, 5],\n",
    "                            [5, 3, 3, 3, 3, 5],\n",
    "                            [7, 5, 5, 5, 5, 7]\n",
    "                          ]\n",
    "# regularizer !\n",
    "l2_val = 0.00005\n",
    "# Adam optimizer !\n",
    "adam_learning_rate = 0.0004\n",
    "\n",
    "## ===================  COPIED CODE ==========================\n",
    "#\n",
    "#  TENSORBOARD VISUALIZATION FOR SHARPNESS AND (Peak Signal to Noise Ratio){PSNR}\n",
    "#\n",
    "\n",
    "def psnr_error(gen_frames, gt_frames):\n",
    "    \"\"\"\n",
    "    Computes the Peak Signal to Noise Ratio error between the generated images and the ground\n",
    "    truth images.\n",
    "    @param gen_frames: A tensor of shape [batch_size, height, width, 3]. The frames generated by the\n",
    "                       generator model.\n",
    "    @param gt_frames: A tensor of shape [batch_size, height, width, 3]. The ground-truth frames for\n",
    "                      each frame in gen_frames.\n",
    "    @return: A scalar tensor. The mean Peak Signal to Noise Ratio error over each frame in the\n",
    "             batch.\n",
    "    \"\"\"\n",
    "    shape = tf.shape(gen_frames)\n",
    "    num_pixels = tf.to_float(shape[1] * shape[2] * shape[3])\n",
    "    square_diff = tf.square(gt_frames - gen_frames)\n",
    "\n",
    "    batch_errors = 10 * log10(1 / ((1 / num_pixels) * tf.reduce_sum(square_diff, [1, 2, 3])))\n",
    "    return tf.reduce_mean(batch_errors)\n",
    "\n",
    "def sharp_diff_error(gen_frames, gt_frames):\n",
    "    \"\"\"\n",
    "    Computes the Sharpness Difference error between the generated images and the ground truth\n",
    "    images.\n",
    "    @param gen_frames: A tensor of shape [batch_size, height, width, 3]. The frames generated by the\n",
    "                       generator model.\n",
    "    @param gt_frames: A tensor of shape [batch_size, height, width, 3]. The ground-truth frames for\n",
    "                      each frame in gen_frames.\n",
    "    @return: A scalar tensor. The Sharpness Difference error over each frame in the batch.\n",
    "    \"\"\"\n",
    "    shape = tf.shape(gen_frames)\n",
    "    num_pixels = tf.to_float(shape[1] * shape[2] * shape[3])\n",
    "\n",
    "    # gradient difference\n",
    "    # create filters [-1, 1] and [[1],[-1]] for diffing to the left and down respectively.\n",
    "    # TODO: Could this be simplified with one filter [[-1, 2], [0, -1]]?\n",
    "    pos = tf.constant(np.identity(3), dtype=tf.float32)\n",
    "    neg = -1 * pos\n",
    "    filter_x = tf.expand_dims(tf.pack([neg, pos]), 0)  # [-1, 1]\n",
    "    filter_y = tf.pack([tf.expand_dims(pos, 0), tf.expand_dims(neg, 0)])  # [[1],[-1]]\n",
    "    strides = [1, 1, 1, 1]  # stride of (1, 1)\n",
    "    padding = 'SAME'\n",
    "\n",
    "    gen_dx = tf.abs(tf.nn.conv2d(gen_frames, filter_x, strides, padding=padding))\n",
    "    gen_dy = tf.abs(tf.nn.conv2d(gen_frames, filter_y, strides, padding=padding))\n",
    "    gt_dx = tf.abs(tf.nn.conv2d(gt_frames, filter_x, strides, padding=padding))\n",
    "    gt_dy = tf.abs(tf.nn.conv2d(gt_frames, filter_y, strides, padding=padding))\n",
    "\n",
    "    gen_grad_sum = gen_dx + gen_dy\n",
    "    gt_grad_sum = gt_dx + gt_dy\n",
    "\n",
    "    grad_diff = tf.abs(gt_grad_sum - gen_grad_sum)\n",
    "\n",
    "    batch_errors = 10 * log10(1 / ((1 / num_pixels) * tf.reduce_sum(grad_diff, [1, 2, 3])))\n",
    "    return tf.reduce_mean(batch_errors)\n",
    "\n",
    "## =================== COPIED CODE ENDS ======================\n",
    "\n",
    "\n",
    "def l2_loss(generated_frames, expected_frames):\n",
    "    losses = []\n",
    "    for each_scale_gen_frames, each_scale_exp_frames in zip(generated_frames, expected_frames):\n",
    "        losses.append(tf.nn.l2_loss(tf.subtract(each_scale_gen_frames, each_scale_exp_frames)))\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.stack(losses))\n",
    "    return loss\n",
    "\n",
    "def gdl_loss(generated_frames, expected_frames, alpha=2):\n",
    "    \"\"\"\n",
    "    difference with side pixel and below pixel\n",
    "    \"\"\"\n",
    "    scale_losses = []\n",
    "    for i in xrange(len(generated_frames)):\n",
    "        # create filters [-1, 1] and [[1],[-1]] for diffing to the left and down respectively.\n",
    "        pos = tf.constant(np.identity(3), dtype=tf.float32)\n",
    "        neg = -1 * pos\n",
    "        filter_x = tf.expand_dims(tf.stack([neg, pos]), 0)  # [-1, 1]\n",
    "        filter_y = tf.stack([tf.expand_dims(pos, 0), tf.expand_dims(neg, 0)])  # [[1],[-1]]\n",
    "        strides = [1, 1, 1, 1]  # stride of (1, 1)\n",
    "        padding = 'SAME'\n",
    "\n",
    "        gen_dx = tf.abs(tf.nn.conv2d(generated_frames[i], filter_x, strides, padding=padding))\n",
    "        gen_dy = tf.abs(tf.nn.conv2d(generated_frames[i], filter_y, strides, padding=padding))\n",
    "        gt_dx = tf.abs(tf.nn.conv2d(expected_frames[i], filter_x, strides, padding=padding))\n",
    "        gt_dy = tf.abs(tf.nn.conv2d(expected_frames[i], filter_y, strides, padding=padding))\n",
    "\n",
    "        grad_diff_x = tf.abs(gt_dx - gen_dx)\n",
    "        grad_diff_y = tf.abs(gt_dy - gen_dy)\n",
    "\n",
    "        scale_losses.append(tf.reduce_sum((grad_diff_x ** alpha + grad_diff_y ** alpha)))\n",
    "\n",
    "    # condense into one tensor and avg\n",
    "    return tf.reduce_mean(tf.stack(scale_losses))\n",
    "\n",
    "def total_loss(generated_frames, expected_frames, lambda_gdl=1.0, lambda_l2=1.0):\n",
    "    total_loss_cal = (lambda_gdl * gdl_loss(generated_frames, expected_frames) + \n",
    "                     lambda_l2 * l2_loss(generated_frames, expected_frames))\n",
    "    return total_loss_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GenerativeNetwork:\n",
    "    def __init__(self,heigth_train, width_train, heigth_test, width_test, scale_level_feature_maps, scale_level_kernel_size):\n",
    "        self.heigth_train = heigth_train\n",
    "        self.width_train = width_train\n",
    "        self.heigth_test = heigth_test\n",
    "        self.width_test = width_test\n",
    "\n",
    "        self.scale_level_feature_maps = scale_level_feature_maps\n",
    "        self.scale_level_kernel_size = scale_level_kernel_size\n",
    "        self.len_scale = len(self.scale_level_kernel_size)\n",
    "        assert len(self.scale_level_feature_maps) == len(self.scale_level_kernel_size), \"Length should be equal !\"\n",
    "        \n",
    "        # Placeholders for inputs and outputs ... !\n",
    "        self.input_train = tf.placeholder(dtype=tf.float32, shape=[None, self.heigth_train, self.width_train, time_frames_to_consider * image_channels])\n",
    "        self.output_train = tf.placeholder(dtype=tf.float32, shape=[None, self.heigth_train, self.width_train, image_channels])\n",
    "        self.input_test = tf.placeholder(dtype=tf.float32, shape=[None, self.heigth_test, self.width_test, time_frames_to_consider * image_channels])\n",
    "        self.output_test = tf.placeholder(dtype=tf.float32, shape=[None, self.heigth_test, self.width_test, image_channels])\n",
    "        \n",
    "        self.each_scale_predication_train = []\n",
    "        self.each_scale_ground_truth_train = []\n",
    "        self.each_scale_predication_test = []\n",
    "        self.each_scale_ground_truth_test = []\n",
    "        \n",
    "        self.create_graph(self.input_train, self.output_train, heigth_train, width_train, \n",
    "                          self.each_scale_predication_train, \n",
    "                          self.each_scale_ground_truth_train,\n",
    "                          reuse=None)\n",
    "        \n",
    "        # reuse graph at time of test !\n",
    "        self.create_graph(self.input_train, self.output_train, heigth_test, width_test, \n",
    "                          self.each_scale_predication_test,\n",
    "                          self.each_scale_ground_truth_test,\n",
    "                          reuse=True)\n",
    "        \n",
    "        self.loss()\n",
    "        \n",
    "        # print self.each_scale_predication_train\n",
    "        # print self.each_scale_ground_truth_train\n",
    "        # print self.each_scale_predication_test\n",
    "        # print self.each_scale_ground_truth_test\n",
    "        \n",
    "    def rescale_image(self, scaling_factor, heigth, width, input_data, ground_truths, last_generated_frame):\n",
    "        \"\"\"\n",
    "        scaling_factor, heigth, width = values\n",
    "        input_data, ground_truths = Tensors\n",
    "        \"\"\"\n",
    "        rescaled_heigth = int(scaling_factor * heigth)\n",
    "        rescaled_width = int(scaling_factor * width)\n",
    "        assert rescaled_heigth != 0 and rescaled_width != 0, \"scaling factor should not be zero !\"\n",
    "        input_reshaped = tf.image.resize_images(input_data, [rescaled_heigth, rescaled_width])\n",
    "        ground_truths_reshaped = tf.image.resize_images(ground_truths, [rescaled_heigth, rescaled_width])\n",
    "        last_generated_frame_reshaped = None\n",
    "        if last_generated_frame!=None:\n",
    "            last_generated_frame_reshaped = tf.image.resize_images(last_generated_frame, [rescaled_heigth, rescaled_width])\n",
    "        return (input_reshaped, ground_truths_reshaped, last_generated_frame_reshaped)\n",
    "    \n",
    "    def create_graph(self, input_data, ground_truths, heigth, width, \n",
    "                     predicated_at_each_scale_tensor, ground_truth_at_each_scale_tensor, reuse):\n",
    "                \n",
    "        # for each scale ... \n",
    "        for each_scale in range(self.len_scale):\n",
    "            conv_counter = 0 \n",
    "            with tf.variable_scope('scale_'+str(each_scale),reuse=reuse):\n",
    "                # scaling create [1/64, 1/32, 1/16, 1/4]\n",
    "                scaling_factor = 1.0 / (2**(self.len_scale - 1 - each_scale))\n",
    "                last_generated_frame = None\n",
    "                if each_scale > 0:\n",
    "                    last_generated_frame = predicated_at_each_scale_tensor[each_scale-1]\n",
    "                \n",
    "                input_reshaped, ground_truths_reshaped, last_generated_frame_reshaped = self.rescale_image(scaling_factor, heigth, width, input_data, ground_truths, last_generated_frame)\n",
    "                \n",
    "                # append last scale output \n",
    "                if each_scale > 0:\n",
    "                    input_reshaped = tf.concat([input_reshaped, last_generated_frame_reshaped],axis=3)\n",
    "                \n",
    "                # print (input_reshaped, ground_truths_reshaped)\n",
    "                predication = input_reshaped\n",
    "                \n",
    "                # for each conv layers in that scale ... \n",
    "                feature_maps = scale_level_feature_maps[each_scale]\n",
    "                kernel_size = scale_level_kernel_size[each_scale]\n",
    "                \n",
    "                assert len(feature_maps)==len(kernel_size), \"Length should be equal !\"\n",
    "                for index, (each_filter, each_kernel) in enumerate(zip(feature_maps, kernel_size)): \n",
    "                    with tf.variable_scope('conv_'+str(conv_counter),reuse=reuse):\n",
    "                        conv_counter += 1\n",
    "                        activiation = tf.nn.relu\n",
    "                        # last layer tanh !\n",
    "                        if index==(len(kernel_size)-1):\n",
    "                            activiation = tf.nn.tanh\n",
    "                        predication = slim.conv2d(predication, each_filter, [each_kernel, each_kernel], \n",
    "                                              weights_initializer=trunc_normal(0.01),\n",
    "                                              weights_regularizer=regularizers.l2_regularizer(l2_val),\n",
    "                                              activation_fn=activiation)\n",
    "                \n",
    "                        \n",
    "                # APPEND LAST GENERATED FRAME\n",
    "                predicated_at_each_scale_tensor.append(predication)\n",
    "                ground_truth_at_each_scale_tensor.append(ground_truths_reshaped)\n",
    "                \n",
    "    def loss(self):\n",
    "        self.combined_loss = total_loss(self.each_scale_predication_train, self.each_scale_ground_truth_train)\n",
    "        self.optimizer = tf.train.AdamOptimizer(adam_learning_rate)\n",
    "        global_step = tf.Variable(0,name=\"global_step_var\",trainable=False)\n",
    "        self.step = self.optimizer.minimize(self.combined_loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GenerativeNetwork(heigth_train, width_train, heigth_test, width_test, scale_level_feature_maps, scale_level_kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.concat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slim.conv2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pos = tf.constant(np.identity(3), dtype=tf.float32)\n",
    "neg = -1 * pos\n",
    "sess = tf.Session()\n",
    "print sess.run(pos)\n",
    "print sess.run(tf.expand_dims(tf.stack([neg, pos]), 0))\n",
    "print sess.run(tf.expand_dims(pos, 0))\n",
    "print sess.run(tf.stack([tf.expand_dims(pos, 0), tf.expand_dims(neg, 0)]))\n",
    "x = tf.constant(np.arange(4*2*3).reshape(1,4,2,3), dtype=tf.float32)\n",
    "y = tf.constant(np.arange(24,24+4*2*3).reshape(1,4,2,3), dtype=tf.float32)\n",
    "print sess.run(x)\n",
    "print (\"----\")\n",
    "print sess.run(y)\n",
    "pos = tf.constant(np.identity(3), dtype=tf.float32)\n",
    "neg = -1 * pos\n",
    "filter_x = tf.expand_dims(tf.stack([neg, pos]), 0)  # [-1, 1]\n",
    "filter_y = tf.stack([tf.expand_dims(pos, 0), tf.expand_dims(neg, 0)])  # [[1],[-1]]\n",
    "strides = [1, 1, 1, 1]  # stride of (1, 1)\n",
    "padding = 'SAME'\n",
    "print (sess.run(filter_y)).shape\n",
    "sess.run(tf.nn.conv2d(x, filter_x, strides, padding=padding))\n",
    "sess.run(tf.nn.conv2d(x, filter_y, strides, padding=padding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
