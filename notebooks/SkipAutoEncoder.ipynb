{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.contrib.layers.python.layers import regularizers\n",
    "\n",
    "# module_path = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"..\")\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "# from datasets.batch_generator import datasets\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "tf.reset_default_graph()\n",
    "trunc_normal = lambda stddev: init_ops.truncated_normal_initializer(0.0, stddev)\n",
    "\n",
    "# Contants\n",
    "image_channels = 3\n",
    "time_frames_to_consider = 4\n",
    "time_frames_to_predict = 4\n",
    "interval=4 # frames to jump !\n",
    "heigth_train= 64\n",
    "width_train= 64\n",
    "custom_test_size=[160,210]\n",
    "heigth_test, width_test = custom_test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regularizer !\n",
    "l2_val = 0.00005\n",
    "# Adam optimizer !\n",
    "adam_learning_rate = 0.0004\n",
    "# Tensorboard images to show\n",
    "batch_size = 8\n",
    "number_of_images_to_show = 4\n",
    "assert number_of_images_to_show <= batch_size, \"images to show should be less !\"\n",
    "timesteps=16\n",
    "file_path = \"\"\n",
    "data_folder = os.path.join(file_path, \"../../data/\")\n",
    "log_dir_file_path = os.path.join(file_path, \"../../logs/\")\n",
    "model_save_file_path = os.path.join(file_path, \"../../checkpoint/\")\n",
    "output_video_save_file_path = os.path.join(file_path, \"../../output/\")\n",
    "iterations = \"iterations/\"\n",
    "best = \"best/\"\n",
    "checkpoint_iterations = 100\n",
    "best_model_iterations = 100\n",
    "test_model_iterations = 5\n",
    "best_loss = float(\"inf\")\n",
    "heigth, width = heigth_train, width_train\n",
    "channels = 3\n",
    "assert timesteps>=time_frames_to_consider and timesteps>=time_frames_to_predict, \"time steps must be greater !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heigth_train = 64\n",
    "width_train = 64\n",
    "heigth_test = 160\n",
    "width_test = 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders for inputs and outputs ... !\n",
    "input_train = tf.placeholder(dtype=tf.float32, shape=[None, heigth_train, width_train, time_frames_to_consider * image_channels])\n",
    "output_train = tf.placeholder(dtype=tf.float32, shape=[None, heigth_train, width_train, image_channels])\n",
    "input_test = tf.placeholder(dtype=tf.float32, shape=[None, heigth_test, width_test, time_frames_to_consider * image_channels])\n",
    "output_test = tf.placeholder(dtype=tf.float32, shape=[None, heigth_test, width_test, image_channels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#slim.conv2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = [32,64,128,256,512]\n",
    "kernel_size = [3,3,3,3,3]\n",
    "stride_size = [1,2,2,2,2]\n",
    "assert len(kernel_size) == len(feature_maps)==len(stride_size), \"lens must be equal\"\n",
    "\n",
    "def conv_layer(conv_input,reuse=None):\n",
    "        layers_for_skip = []\n",
    "        net = conv_input\n",
    "        with tf.variable_scope('conv_autoencoder',reuse=reuse):\n",
    "            for i, (each_feat_map, each_kernel_size, each_stride) in enumerate(zip(feature_maps, kernel_size, stride_size)):\n",
    "                net = slim.conv2d(net, each_feat_map, [each_kernel_size, each_kernel_size], stride=each_stride, \n",
    "                                  scope='conv_'+str(i), weights_initializer=trunc_normal(0.01),\n",
    "                                  weights_regularizer=regularizers.l2_regularizer(l2_val))\n",
    "                layers_for_skip.append(net)\n",
    "            \n",
    "            return net, layers_for_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_op, layers_to_skip = conv_layer(input_train, reuse=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[256, 128, 64, 32, 3]\n",
      "[3, 3, 3, 3, 3]\n",
      "[2, 2, 2, 2, 1]\n",
      "[<tf.Tensor 'conv_autoencoder/conv_3/Relu:0' shape=(?, 8, 8, 256) dtype=float32>, <tf.Tensor 'conv_autoencoder/conv_2/Relu:0' shape=(?, 16, 16, 128) dtype=float32>, <tf.Tensor 'conv_autoencoder/conv_1/Relu:0' shape=(?, 32, 32, 64) dtype=float32>, <tf.Tensor 'conv_autoencoder/conv_0/Relu:0' shape=(?, 64, 64, 32) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "feature_maps = feature_maps[:-1][::-1] + [image_channels,]\n",
    "kernel_size = kernel_size[:-1][::-1]  + [3,]\n",
    "stride_size = stride_size[1:][::-1] + [1,]\n",
    "print feature_maps\n",
    "print kernel_size\n",
    "print stride_size\n",
    "print layers_to_skip[:-1][::-1]\n",
    "assert len(kernel_size) == len(feature_maps)==len(stride_size), \"lens must be equal\"\n",
    "\n",
    "def deconv_layer(deconv_input, layers_to_skip, reuse=None):\n",
    "    layers_to_skip = layers_to_skip[:-1][::-1]\n",
    "    net = deconv_input\n",
    "    with tf.variable_scope('deconv_autoencoder',reuse=reuse):\n",
    "        for i, (each_feat_map, each_kernel_size, each_stride) in enumerate(zip(feature_maps, kernel_size, stride_size)):\n",
    "                activation = tf.nn.relu\n",
    "                if i==(len(stride_size)-1):\n",
    "                    # last layer !\n",
    "                    activation = tf.nn.tanh\n",
    "                if i>0:\n",
    "                    # not first layer !\n",
    "                    net = tf.concat([net,layers_to_skip[i-1]],axis=3)\n",
    "                    print \"concated \",i-1,\" \", net\n",
    "                net = slim.conv2d_transpose(net, each_feat_map, [each_kernel_size, each_kernel_size], stride=each_stride,\n",
    "                                  activation_fn = activation,\n",
    "                                  scope='deconv_'+str(i), weights_initializer=trunc_normal(0.01),\n",
    "                                  weights_regularizer=regularizers.l2_regularizer(l2_val))\n",
    "                print net\n",
    "                \n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"deconv_autoencoder/deconv_0/Relu:0\", shape=(?, 8, 8, 256), dtype=float32)\n",
      "concated  0   Tensor(\"deconv_autoencoder/concat:0\", shape=(?, 8, 8, 512), dtype=float32)\n",
      "Tensor(\"deconv_autoencoder/deconv_1/Relu:0\", shape=(?, 16, 16, 128), dtype=float32)\n",
      "concated  1   Tensor(\"deconv_autoencoder/concat_1:0\", shape=(?, 16, 16, 256), dtype=float32)\n",
      "Tensor(\"deconv_autoencoder/deconv_2/Relu:0\", shape=(?, 32, 32, 64), dtype=float32)\n",
      "concated  2   Tensor(\"deconv_autoencoder/concat_2:0\", shape=(?, 32, 32, 128), dtype=float32)\n",
      "Tensor(\"deconv_autoencoder/deconv_3/Relu:0\", shape=(?, 64, 64, 32), dtype=float32)\n",
      "concated  3   Tensor(\"deconv_autoencoder/concat_3:0\", shape=(?, 64, 64, 64), dtype=float32)\n",
      "Tensor(\"deconv_autoencoder/deconv_4/Tanh:0\", shape=(?, 64, 64, 3), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'deconv_autoencoder/deconv_4/Tanh:0' shape=(?, 64, 64, 3) dtype=float32>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deconv_layer(encoded_op, layers_to_skip,reuse=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv_autoencoder/conv_4/Relu:0\", shape=(?, 4, 4, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print encoded_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#====================  COPIED CODE ===============================================\n",
    "#\n",
    "#  TENSORBOARD VISUALIZATION FOR SHARPNESS AND (Peak Signal to Noise Ratio){PSNR}\n",
    "#=================================================================================\n",
    "def log10(t):\n",
    "    \"\"\"\n",
    "    Calculates the base-10 log of each element in t.\n",
    "    @param t: The tensor from which to calculate the base-10 log.\n",
    "    @return: A tensor with the base-10 log of each element in t.\n",
    "    \"\"\"\n",
    "    numerator = tf.log(t)\n",
    "    denominator = tf.log(tf.constant(10, dtype=numerator.dtype))\n",
    "    return numerator / denominator\n",
    "    \n",
    "def psnr_error(gen_frames, gt_frames):\n",
    "    \"\"\"\n",
    "    Computes the Peak Signal to Noise Ratio error between the generated images and the ground\n",
    "    truth images.\n",
    "    @param gen_frames: A tensor of shape [batch_size, height, width, 3]. The frames generated by the\n",
    "                       generator model.\n",
    "    @param gt_frames: A tensor of shape [batch_size, height, width, 3]. The ground-truth frames for\n",
    "                      each frame in gen_frames.\n",
    "    @return: A scalar tensor. The mean Peak Signal to Noise Ratio error over each frame in the\n",
    "             batch.\n",
    "    \"\"\"\n",
    "    shape = tf.shape(gen_frames)\n",
    "    num_pixels = tf.to_float(shape[1] * shape[2] * shape[3])\n",
    "    square_diff = tf.square(gt_frames - gen_frames)\n",
    "\n",
    "    batch_errors = 10 * log10(1 / ((1 / num_pixels) * tf.reduce_sum(square_diff, [1, 2, 3])))\n",
    "    return tf.reduce_mean(batch_errors)\n",
    "\n",
    "def sharp_diff_error(gen_frames, gt_frames):\n",
    "    \"\"\"\n",
    "    Computes the Sharpness Difference error between the generated images and the ground truth\n",
    "    images.\n",
    "    @param gen_frames: A tensor of shape [batch_size, height, width, 3]. The frames generated by the\n",
    "                       generator model.\n",
    "    @param gt_frames: A tensor of shape [batch_size, height, width, 3]. The ground-truth frames for\n",
    "                      each frame in gen_frames.\n",
    "    @return: A scalar tensor. The Sharpness Difference error over each frame in the batch.\n",
    "    \"\"\"\n",
    "    shape = tf.shape(gen_frames)\n",
    "    num_pixels = tf.to_float(shape[1] * shape[2] * shape[3])\n",
    "\n",
    "    # gradient difference\n",
    "    # create filters [-1, 1] and [[1],[-1]] for diffing to the left and down respectively.\n",
    "    # TODO: Could this be simplified with one filter [[-1, 2], [0, -1]]?\n",
    "    pos = tf.constant(np.identity(3), dtype=tf.float32)\n",
    "    neg = -1 * pos\n",
    "    filter_x = tf.expand_dims(tf.stack([neg, pos]), 0)  # [-1, 1]\n",
    "    filter_y = tf.stack([tf.expand_dims(pos, 0), tf.expand_dims(neg, 0)])  # [[1],[-1]]\n",
    "    strides = [1, 1, 1, 1]  # stride of (1, 1)\n",
    "    padding = 'SAME'\n",
    "\n",
    "    gen_dx = tf.abs(tf.nn.conv2d(gen_frames, filter_x, strides, padding=padding))\n",
    "    gen_dy = tf.abs(tf.nn.conv2d(gen_frames, filter_y, strides, padding=padding))\n",
    "    gt_dx = tf.abs(tf.nn.conv2d(gt_frames, filter_x, strides, padding=padding))\n",
    "    gt_dy = tf.abs(tf.nn.conv2d(gt_frames, filter_y, strides, padding=padding))\n",
    "\n",
    "    gen_grad_sum = gen_dx + gen_dy\n",
    "    gt_grad_sum = gt_dx + gt_dy\n",
    "\n",
    "    grad_diff = tf.abs(gt_grad_sum - gen_grad_sum)\n",
    "\n",
    "    batch_errors = 10 * log10(1 / ((1 / num_pixels) * tf.reduce_sum(grad_diff, [1, 2, 3])))\n",
    "    return tf.reduce_mean(batch_errors)\n",
    "\n",
    "## =================== COPIED CODE ENDS ======================\n",
    "\n",
    "\n",
    "def l2_loss(generated_frames, expected_frames):\n",
    "    losses = []\n",
    "    for each_scale_gen_frames, each_scale_exp_frames in zip(generated_frames, expected_frames):\n",
    "        losses.append(tf.nn.l2_loss(tf.subtract(each_scale_gen_frames, each_scale_exp_frames)))\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.stack(losses))\n",
    "    return loss\n",
    "\n",
    "def gdl_loss(generated_frames, expected_frames, alpha=2):\n",
    "    \"\"\"\n",
    "    difference with side pixel and below pixel\n",
    "    \"\"\"\n",
    "    scale_losses = []\n",
    "    for i in xrange(len(generated_frames)):\n",
    "        # create filters [-1, 1] and [[1],[-1]] for diffing to the left and down respectively.\n",
    "        pos = tf.constant(np.identity(3), dtype=tf.float32)\n",
    "        neg = -1 * pos\n",
    "        filter_x = tf.expand_dims(tf.stack([neg, pos]), 0)  # [-1, 1]\n",
    "        filter_y = tf.stack([tf.expand_dims(pos, 0), tf.expand_dims(neg, 0)])  # [[1],[-1]]\n",
    "        strides = [1, 1, 1, 1]  # stride of (1, 1)\n",
    "        padding = 'SAME'\n",
    "\n",
    "        gen_dx = tf.abs(tf.nn.conv2d(generated_frames[i], filter_x, strides, padding=padding))\n",
    "        gen_dy = tf.abs(tf.nn.conv2d(generated_frames[i], filter_y, strides, padding=padding))\n",
    "        gt_dx = tf.abs(tf.nn.conv2d(expected_frames[i], filter_x, strides, padding=padding))\n",
    "        gt_dy = tf.abs(tf.nn.conv2d(expected_frames[i], filter_y, strides, padding=padding))\n",
    "\n",
    "        grad_diff_x = tf.abs(gt_dx - gen_dx)\n",
    "        grad_diff_y = tf.abs(gt_dy - gen_dy)\n",
    "\n",
    "        scale_losses.append(tf.reduce_sum((grad_diff_x ** alpha + grad_diff_y ** alpha)))\n",
    "\n",
    "    # condense into one tensor and avg\n",
    "    return tf.reduce_mean(tf.stack(scale_losses))\n",
    "\n",
    "def total_loss(generated_frames, expected_frames, lambda_gdl=1.0, lambda_l2=1.0):\n",
    "    total_loss_cal = (lambda_gdl * gdl_loss(generated_frames, expected_frames) + \n",
    "                     lambda_l2 * l2_loss(generated_frames, expected_frames))\n",
    "    return total_loss_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipAutoEncoder:\n",
    "    def __init__(self, heigth_train, width_train, heigth_test, width_test):\n",
    "        \n",
    "        self.heigth_train = heigth_train\n",
    "        self.width_train = width_train\n",
    "        self.heigth_test = heigth_test\n",
    "        self.width_test = width_test\n",
    "\n",
    "        self.feature_maps = [32,64,128,256,512]\n",
    "        self.kernel_size = [3,3,3,3,3]\n",
    "        self.stride_size = [1,2,2,2,2]\n",
    "        assert len(self.kernel_size) == len(self.feature_maps)==len(self.stride_size), \"lens must be equal\"\n",
    "        \n",
    "        # Placeholders for inputs and outputs ... !\n",
    "        self.input_train = tf.placeholder(dtype=tf.float32, shape=[None, self.heigth_train, self.width_train, time_frames_to_consider * image_channels])\n",
    "        self.output_train = tf.placeholder(dtype=tf.float32, shape=[None, self.heigth_train, self.width_train, image_channels])\n",
    "        self.input_test = tf.placeholder(dtype=tf.float32, shape=[None, self.heigth_test, self.width_test, time_frames_to_consider * image_channels])\n",
    "        self.output_test = tf.placeholder(dtype=tf.float32, shape=[None, self.heigth_test, self.width_test, image_channels])\n",
    "        \n",
    "        self.model_output = self.create_graph(self.input_train, self.output_train,reuse=None)\n",
    "        \n",
    "        # reuse graph at time of test !\n",
    "        self.model_output_test = self.create_graph(self.input_test, self.output_test,reuse=True)\n",
    "        \n",
    "        self.loss()\n",
    "        self.tf_summary()\n",
    "\n",
    "    def conv_layer(self, conv_input,reuse):\n",
    "        layers_for_skip = []\n",
    "        net = conv_input\n",
    "        with tf.variable_scope('conv_autoencoder',reuse=reuse):\n",
    "            for i, (each_feat_map, each_kernel_size, each_stride) in enumerate(zip(self.feature_maps, self.kernel_size, self.stride_size)):\n",
    "                net = slim.conv2d(net, each_feat_map, [each_kernel_size, each_kernel_size], stride=each_stride, \n",
    "                                  scope='conv_'+str(i), weights_initializer=trunc_normal(0.01),\n",
    "                                  weights_regularizer=regularizers.l2_regularizer(l2_val))\n",
    "                layers_for_skip.append(net)\n",
    "            \n",
    "            return net, layers_for_skip\n",
    "\n",
    "    def deconv_layer(self, deconv_input, layers_to_skip, reuse):\n",
    "        feature_maps = self.feature_maps[:-1][::-1] + [image_channels,]\n",
    "        kernel_size = self.kernel_size[:-1][::-1]  + [3,]\n",
    "        stride_size = self.stride_size[1:][::-1] + [1,]\n",
    "        assert len(kernel_size) == len(feature_maps)==len(stride_size), \"lens must be equal\"\n",
    "        layers_to_skip_d = layers_to_skip[:-1][::-1]\n",
    "        net = deconv_input\n",
    "        with tf.variable_scope('deconv_autoencoder',reuse=reuse):\n",
    "            for i, (each_feat_map, each_kernel_size, each_stride) in enumerate(zip(feature_maps, kernel_size, stride_size)):\n",
    "                    activation = tf.nn.relu\n",
    "                    if i==(len(stride_size)-1):\n",
    "                        # last layer !\n",
    "                        activation = tf.nn.tanh\n",
    "                    if i>0:\n",
    "                        # not first layer !\n",
    "                        net = tf.concat([net,layers_to_skip_d[i-1]],axis=3)\n",
    "                    net = slim.conv2d_transpose(net, each_feat_map, [each_kernel_size, each_kernel_size], stride=each_stride,\n",
    "                                      activation_fn = activation,\n",
    "                                      scope='deconv_'+str(i), weights_initializer=trunc_normal(0.01),\n",
    "                                      weights_regularizer=regularizers.l2_regularizer(l2_val))\n",
    "                    #print net\n",
    "            return net\n",
    "    \n",
    "    def create_graph(self, input_data, ground_truths, reuse):\n",
    "        encoded_op, layers_to_skip = self.conv_layer(input_data, reuse=reuse)\n",
    "        #print encoded_op, layers_to_skip\n",
    "        return self.deconv_layer(encoded_op, layers_to_skip,reuse=reuse)\n",
    "    \n",
    "    def loss(self):        \n",
    "        # gdl and l2 loss !\n",
    "        self.combined_loss = total_loss([self.model_output], [self.output_train])\n",
    "        self.optimizer = tf.train.AdamOptimizer(adam_learning_rate)\n",
    "        global_step = tf.Variable(0,name=\"global_step_var\",trainable=False)\n",
    "        self.step = self.optimizer.minimize(self.combined_loss, global_step=global_step)\n",
    "\n",
    "    def tf_summary(self):\n",
    "        train_loss = tf.summary.scalar(\"gen_train_loss\", self.combined_loss)\n",
    "        val_loss = tf.summary.scalar(\"gen_val_loss\", self.combined_loss)\n",
    "        with tf.variable_scope('image_measures'):\n",
    "            psnr_error_train = psnr_error(self.model_output, self.output_train)\n",
    "            psnr_error_train_s = tf.summary.scalar(\"train_psnr\",psnr_error_train)\n",
    "            psnr_error_val_s = tf.summary.scalar(\"val_psnr\",psnr_error_train)\n",
    "\n",
    "\n",
    "            sharpdiff_error_train = sharp_diff_error(self.model_output,self.output_train)\n",
    "            sharpdiff_error_train_s = tf.summary.scalar(\"train_shardiff\",sharpdiff_error_train)\n",
    "            sharpdiff_error_val_s = tf.summary.scalar(\"val_shardiff\",sharpdiff_error_train)\n",
    "\n",
    "            images_to_show_train = []\n",
    "            images_to_show_val = []\n",
    "\n",
    "            images_to_show_train.append(tf.summary.image('train_output', self.model_output,\n",
    "                         number_of_images_to_show))\n",
    "            images_to_show_train.append(tf.summary.image('train_ground_truth', self.output_train,\n",
    "                         number_of_images_to_show))\n",
    "            images_to_show_val.append(tf.summary.image('val_output', self.model_output,\n",
    "                         number_of_images_to_show))\n",
    "            images_to_show_val.append(tf.summary.image('val_ground_truth', self.output_train,\n",
    "                         number_of_images_to_show))\n",
    "                \n",
    "\n",
    "            psnr_error_test = psnr_error(self.model_output_test, self.output_test)\n",
    "            psnr_error_test_s = tf.summary.scalar(\"test_psnr\",psnr_error_test)\n",
    "\n",
    "            sharpdiff_error_test = sharp_diff_error(self.model_output_test,self.output_test)\n",
    "            sharpdiff_error_test_s = tf.summary.scalar(\"test_shardiff\",sharpdiff_error_test)\n",
    "\n",
    "            images_to_show_test = []\n",
    "            images_to_show_test.append(tf.summary.image('test_output', self.model_output_test,\n",
    "                         number_of_images_to_show))\n",
    "            images_to_show_test.append(tf.summary.image('test_ground', self.output_test,\n",
    "                         number_of_images_to_show))\n",
    "\n",
    "        self.train_summary_merged = tf.summary.merge([train_loss, psnr_error_train_s, sharpdiff_error_train_s]+images_to_show_train)\n",
    "        self.test_summary_merged = tf.summary.merge([psnr_error_test_s, sharpdiff_error_test_s]+images_to_show_test)\n",
    "        self.val_summary_merged = tf.summary.merge([val_loss, psnr_error_val_s, sharpdiff_error_val_s]+images_to_show_val)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipAutoEncoder(heigth_train, width_train, heigth_test, width_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
